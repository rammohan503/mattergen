# ğŸ“š MatterGen Training: Complete Explanation

## Overview

MatterGen is a **diffusion-based generative model** that learns to generate novel inorganic crystal structures with desired material properties. The training process teaches the model to gradually denoise random structures into realistic crystalline materials.

---

## High-Level Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRAINING PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: DATA PREPARATION
â”œâ”€ Raw CSV (materials database)
â”œâ”€ Convert to preprocessed dataset
â””â”€ Create train/validation splits

Step 2: MODEL INITIALIZATION
â”œâ”€ Load GemNetT backbone
â”œâ”€ Add property embeddings
â””â”€ Set up diffusion schedules

Step 3: TRAINING LOOP (360+ epochs)
â”œâ”€ Add noise to structures (corruption)
â”œâ”€ Train denoiser to predict original structure
â”œâ”€ Apply classifier-free guidance training
â””â”€ Validate on held-out set

Step 4: SAVE CHECKPOINTS
â”œâ”€ Best model weights
â”œâ”€ Configuration file
â””â”€ Training state

Step 5: OPTIONAL: FINE-TUNING
â”œâ”€ Load base model
â”œâ”€ Add new property embeddings (adapter)
â””â”€ Train on new properties only
```

---

## 1. Data Preparation Phase

### Input: Raw Materials Data
```
datasets/mp_20/train.csv
â”œâ”€â”€ Columns: atomic_numbers, lattice, coordinates, formula, ...
â”œâ”€â”€ Properties: dft_band_gap, dft_mag_density, energy_above_hull, ...
â””â”€â”€ ~20,000 structures (MP database)
```

### Conversion to Dataset
```bash
csv-to-dataset --csv-folder datasets/mp_20/ \
               --output-folder datasets/mp_20/processed/ \
               --train-fraction 0.8 \
               --val-fraction 0.1 \
               --test-fraction 0.1
```

**What happens:**
- Convert CSV rows â†’ `ChemGraph` objects
- Split into train (80%), validation (10%), test (10%)
- Precompute graph structures
- Save as PyTorch DataLoader-compatible format

### Output Structure
```
datasets/mp_20/processed/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ chunk_0000.pt  (batched ChemGraph tensors)
â”‚   â”œâ”€â”€ chunk_0001.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â””â”€â”€ chunk_0000.pt
â””â”€â”€ test/
    â””â”€â”€ chunk_0000.pt
```

---

## 2. Core Data Structure: ChemGraph

All structures flow through the **immutable `ChemGraph`** class (extends PyTorch Geometric's `Data`).

### Required Fields
```python
ChemGraph:
â”œâ”€â”€ atomic_numbers: LongTensor[num_atoms]        # Element type (3â†’Li, 8â†’O, 26â†’Fe)
â”œâ”€â”€ pos: Tensor[num_atoms, 3]                    # Fractional coordinates (0-1)
â”œâ”€â”€ cell: Tensor[1, 3, 3]                        # Lattice vectors (3Ã—3 matrix)
â”œâ”€â”€ num_atoms: LongTensor[num_structures]        # Atoms per structure
â””â”€â”€ batch: LongTensor[num_atoms]                 # Graph index (auto-added)

Property Fields (optional):
â”œâ”€â”€ dft_band_gap: Tensor[num_graphs]             # Energy gap (eV)
â”œâ”€â”€ dft_mag_density: Tensor[num_graphs]          # Magnetic moment (Î¼B/cell)
â”œâ”€â”€ energy_above_hull: Tensor[num_graphs]        # Stability (meV/atom)
â””â”€â”€ ... (any registered property)

Internal Flags:
â””â”€â”€ _USE_UNCONDITIONAL_EMBEDDING: Dict            # Which properties to drop during training
```

### Critical Convention: Immutable Modification
```python
# âœ“ CORRECT: Use .replace() method
x_modified = x.replace(pos=new_pos, atomic_numbers=new_atoms)

# âœ— WRONG: In-place assignment forbidden
x.pos = new_pos  # Raises AttributeError - data is frozen!
```

---

## 3. Model Architecture: Three-Layer System

### Layer 1: GemNetT Denoiser
**File:** `mattergen/denoiser.py`

**Purpose:** The backbone "score network" that predicts denoising direction

```
Input: (noisy_structure, time_step, properties)
  â†“
GemNetT Graph Neural Network
  â”œâ”€ Message passing on crystal graph
  â”œâ”€ Equivariant geometric operations
  â””â”€ Property-conditioned attention
  â†“
Output: (predicted_pos, predicted_lattice, predicted_atoms)
```

**Architecture:**
```
- Node features: GemNetT message passing (atom type + position embedding)
- Edge features: Distance, vector direction (SE(3) equivariant)
- Graph pooling: Aggregates to graph-level features
- Output head: Predicts denoising for pos, cell, atomic_numbers separately

Key: SE(3) equivariance ensures predictions are rotation/translation invariant
```

### Layer 2: Property Embeddings
**File:** `mattergen/property_embeddings.py`

**Purpose:** Condition the model on target properties (band gap, magnetic moment, etc.)

```
Property Value (e.g., 2.5 eV)
  â†“
Embedding Network (MLP)
  â†“
Conditioning Vector â†’ Injected into GemNetT attention
```

**Classifier-Free Guidance Training:**
```
During training, randomly set embeddings to ZERO with probability p:
â”œâ”€ With prob (1-p): Use actual property embedding â†’ conditional score
â”œâ”€ With prob p:    Use zero embedding â†’ unconditional score
â””â”€ Result: Model learns BOTH conditional AND unconditional distributions

At inference:
â”œâ”€ guidance_factor = 0.0  â†’ Pure unconditional generation
â”œâ”€ guidance_factor = 1.0  â†’ Pure conditional (property aware)
â””â”€ guidance_factor = 2.0  â†’ Strong conditioning (adhere to property)
```

**Registered Properties** (in `mattergen/common/utils/globals.py`):
```python
PROPERTY_SOURCE_IDS = {
    'chemical_system',           # e.g., "Li-Fe-P-O"
    'dft_band_gap',              # eV
    'dft_mag_density',           # Î¼B/cell
    'energy_above_hull',         # meV/atom
    'dft_bulk_modulus',          # GPa
    'space_group',               # 1-230
    ...
}
```

### Layer 3: Adapter (Optional Fine-tuning)
**File:** `mattergen/adapter.py`

**Purpose:** Add new properties without retraining the entire model

```
Base Model (frozen or fine-tuned slightly)
  â†“
Adapter Layer (small trainable module)
  â”œâ”€ New property embeddings
  â””â”€ Output â†’ injected into base model
```

**Key Insight:** Adapter unconditional embeddings always return **zero**, preserving the base model's unconditional behavior.

---

## 4. Diffusion Process: Training

### What is Diffusion?

**Forward Process (Corruption):** Add noise gradually to real structures
```
Real Structure â†’ Add noise â†’ More noise â†’ ... â†’ Pure Noise
(time=0)                                        (time=T)
```

**Reverse Process (Denoising):** Learn to remove noise step by step
```
Pure Noise â†’ Denoise â†’ Less noise â†’ ... â†’ Real Structure
(time=T)                                   (time=0)
```

**Training Objective:** Predict the structure at time $t$ given noisy version at time $t+\Delta t$

### Multi-Field Diffusion

MatterGen handles **three fields separately** with different corruption schedules:

#### 1. Atom Positions (Continuous)
```
Corruption: Add Gaussian noise to fractional coordinates
Schedule:  Ïƒ(t) = âˆš(Ïƒ_minÂ² + tÂ·(Ïƒ_maxÂ² - Ïƒ_minÂ²))
Loss:      L_pos = ||predicted_pos - original_pos||Â²
```

#### 2. Lattice Vectors (Continuous)
```
Corruption: Add noise to 3Ã—3 cell matrix (log-space for numerical stability)
Schedule:  Different Ïƒ(t) for lattice vs positions
Loss:      L_cell = ||predicted_cell - original_cell||Â²
```

#### 3. Atom Types (Discrete - D3PM)
```
Corruption: Randomly flip atom types (e.g., 1â†’25 or stay same)
Schedule:  Corruption rate increases with time
Loss:      L_atoms = Cross-entropy(predicted_types, original_types)
```

### Loss Computation

**Total Loss:**
```
L_total = Î»_pos Â· L_pos(t) + Î»_cell Â· L_cell(t) + Î»_atoms Â· L_atoms(t)

Where:
- Î»_pos, Î»_cell, Î»_atoms = learnable or fixed weights per field
- Loss is accumulated over all timesteps
- Weighting controls which field to emphasize
```

**File:** `mattergen/diffusion/losses.py`

### Training Loop Algorithm

```python
for epoch in range(360):
    for batch in train_dataloader:
        # 1. Sample random timestep
        t = random_timestep()  # t âˆˆ [0, T]
        
        # 2. Corrupt structures (forward process)
        x_noisy = corrupt(x, t)
        
        # 3. Optional: dropout conditioning (classifier-free guidance)
        if random() < dropout_prob:
            properties = None  # Unconditional
        else:
            properties = batch.properties  # Conditional
        
        # 4. Forward through denoiser
        x_pred = denoiser(x_noisy, t, properties)
        
        # 5. Compute loss
        loss = compute_loss(x_pred, x, t)
        
        # 6. Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # 7. Validation
    val_loss = evaluate(model, val_dataloader)
    if val_loss < best_loss:
        save_checkpoint(model)
```

---

## 5. Configuration System (Hydra)

### Hierarchical Config Structure

```
mattergen/conf/
â”œâ”€â”€ default.yaml                          (top-level)
â”‚   â”œâ”€â”€ data_module: mp_20
â”‚   â”œâ”€â”€ trainer: default
â”‚   â””â”€â”€ lightning_module: default
â”‚
â”œâ”€â”€ data_module/
â”‚   â”œâ”€â”€ mp_20.yaml               â† 20K materials, 10 properties
â”‚   â””â”€â”€ alex_mp_20.yaml          â† Alternative dataset
â”‚
â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ default.yaml             â† PyTorch Lightning config
â”‚       â”œâ”€â”€ max_epochs: 360
â”‚       â”œâ”€â”€ batch_size: 32
â”‚       â””â”€â”€ accumulate_grad_batches: 4
â”‚
â””â”€â”€ lightning_module/
    â”œâ”€â”€ diffusion_module/
    â”‚   â”œâ”€â”€ default.yaml         â† Diffusion params
    â”‚   â””â”€â”€ model/
    â”‚       â”œâ”€â”€ mattergen.yaml   â† GemNetT config
    â”‚       â””â”€â”€ property_embeddings/
    â”‚           â”œâ”€â”€ dft_band_gap.yaml
    â”‚           â”œâ”€â”€ dft_mag_density.yaml
    â”‚           â””â”€â”€ ...
    â”‚
    â””â”€â”€ adapter/
        â””â”€â”€ default.yaml         â† Fine-tuning config
```

### Command-Line Overrides (Hydra Syntax)

```bash
# Train with custom dataset
mattergen-train data_module=alex_mp_20

# Adjust batch size and accumulation
mattergen-train trainer.batch_size=64 trainer.accumulate_grad_batches=2

# Add new configuration
mattergen-train +model.new_param=value

# Remove configuration
mattergen-train ~trainer.logger

# Override nested values
mattergen-train lightning_module.diffusion_module.noise_schedule=cosine
```

---

## 6. Training Workflow: Commands

### Training Base Model (From Scratch)
```bash
mattergen-train data_module=mp_20 \
                 trainer.max_epochs=360 \
                 ~trainer.logger
```

**Expected Results:**
- **Runtime:** 7-14 days on single GPU (80K training steps)
- **Validation Loss:** â‰ˆ 0.4 after 360 epochs
- **Output:** `outputs/singlerun/${date}/${time}/`
  - Checkpoint files (.pt)
  - Configuration file (config.yaml)
  - Metrics log

### Fine-tuning on New Property
```bash
mattergen-finetune adapter.pretrained_name=mattergen_base \
                    data_module=mp_20 \
                    data_module.properties=["my_property"] \
                    +lightning_module/diffusion_module/model/property_embeddings@adapter.adapter.property_embeddings_adapt.my_property=my_property \
                    ~trainer.logger
```

**What happens:**
1. Load base model weights (mattergen_base checkpoint)
2. Freeze most of the model (except adapter)
3. Train ONLY the new property embeddings
4. Runtime: ~1-2 days (much faster than base training)

---

## 7. Key Training Parameters

### Model Hyperparameters
```yaml
Model (GemNetT):
  num_layers: 4                    # Message passing layers
  hidden_dim: 256                  # Channel dimension
  use_position_encoding: true      # Fourier features
  dropout: 0.1                     # Regularization

Diffusion:
  noise_schedule: 'linear'         # or 'cosine', 'sqrt'
  sigma_min: 0.001                 # Min noise level
  sigma_max: 1.0                   # Max noise level
  num_timesteps: 1000              # Discretization steps
  
Conditioning:
  property_embedding_dim: 128      # Property vector size
  unconditional_prob: 0.1          # Classifier-free guidance dropout
```

### Training Hyperparameters
```yaml
Optimizer:
  learning_rate: 1e-3
  weight_decay: 1e-5
  warmup_steps: 1000
  
Batch:
  batch_size: 32                   # Per GPU
  accumulate_grad_batches: 4       # Gradient accumulation
  num_workers: 8                   # Data loading parallelism
  
Validation:
  val_check_interval: 0.5          # Check every 50% of epoch
  patience: 20                      # Early stopping
```

---

## 8. Monitoring Training

### Key Metrics
```
Logged during training:
â”œâ”€â”€ loss/train_total              (combined loss)
â”œâ”€â”€ loss/train_pos                (position prediction)
â”œâ”€â”€ loss/train_cell               (lattice prediction)
â”œâ”€â”€ loss/train_atoms              (atom type prediction)
â”œâ”€â”€ loss/val_total                (validation loss)
â””â”€â”€ learning_rate                 (optimizer LR)
```

### Tensorboard Visualization
```bash
tensorboard --logdir outputs/singlerun/
```

### Expected Learning Curve
```
Epoch 0:    loss â‰ˆ 2.5  (random predictions)
Epoch 50:   loss â‰ˆ 1.2  (structure learning)
Epoch 150:  loss â‰ˆ 0.6  (fine details)
Epoch 360:  loss â‰ˆ 0.4  (converged)
```

---

## 9. Troubleshooting Training

### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| **OOM (Out of Memory)** | Batch too large | Increase `accumulate_grad_batches`, reduce `batch_size` |
| **Loss not decreasing** | Learning rate too low | Increase `learning_rate` (1e-3 to 1e-2) |
| **Overfitting** | Model too large/dropout too low | Increase `dropout`, reduce `hidden_dim` |
| **Slow convergence** | Dataset too small | Use data augmentation or larger dataset |
| **NaN loss** | Exploding gradients | Use gradient clipping, reduce `learning_rate` |

---

## 10. Next Steps After Training

### Generate Structures (Unconditional)
```bash
mattergen-generate results/ \
  --pretrained-name=mattergen_base \
  --batch_size=16 --num_batches=10
```

### Generate with Property Conditioning
```bash
mattergen-generate results/ \
  --pretrained-name=dft_band_gap \
  --properties_to_condition_on="{'dft_band_gap': 2.5}" \
  --diffusion_guidance_factor=2.0
```

### Evaluate Generated Structures
```bash
mattergen-evaluate results/generated_crystals.extxyz \
  --compute-metrics=True \
  --mp-api-key=${MP_API_KEY}
```

---

## Summary

**MatterGen Training = Learning to Denoise Structures**

1. **Data:** Load crystal structures from Materials Project
2. **Model:** GemNetT denoiser + property embeddings
3. **Process:** Add noise â†’ train to remove it
4. **Conditioning:** Classifier-free guidance for property control
5. **Result:** Model learns to generate diverse, realistic structures

**Training Timeline:** 7-14 days â†’ produces a model that can generate millions of novel crystal structures!


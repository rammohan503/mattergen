# MatterGen: Complete Architecture & Workflow Guide

## Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture Overview](#architecture-overview)
3. [Core Technologies & Libraries](#core-technologies--libraries)
4. [Data Flow & Processing](#data-flow--processing)
5. [Key Classes & Components](#key-classes--components)
6. [Configuration System (Hydra)](#configuration-system-hydra)
7. [Execution Workflows](#execution-workflows)
8. [Important Files & Their Roles](#important-files--their-roles)

---

## Project Overview

**MatterGen** is a generative diffusion model for inorganic materials design that:
- Generates crystal structures across the periodic table
- Can be fine-tuned to condition on various properties (band gap, chemical system, space group, etc.)
- Uses graph neural networks to process crystal data
- Employs denoising diffusion probabilistic models (DDPM) for generation

### Key Capabilities
```
TRAIN      â†’ Learn from crystal database
GENERATE   â†’ Sample new crystal structures
EVALUATE   â†’ Assess quality of generated materials
FINETUNE   â†’ Adapt base model to specific properties
```

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MatterGen Full Stack                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          ENTRY POINTS (scripts/)                         â”‚   â”‚
â”‚  â”‚  generate.py, train/run.py, finetune.py, evaluate.py     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      HYDRA CONFIGURATION MANAGER (conf/)                 â”‚   â”‚
â”‚  â”‚  - Merges YAML configs with CLI arguments                â”‚   â”‚
â”‚  â”‚  - Instantiates DictConfig objects                       â”‚   â”‚
â”‚  â”‚  - Handles composable configuration                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    GENERATOR/TRAINING ORCHESTRATOR                       â”‚   â”‚
â”‚  â”‚  CrystalGenerator | DiffusionLightningModule             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚        â”‚                  â”‚                  â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  DATA LAYER    â”‚ â”‚ DIFFUSION     â”‚ â”‚ GEM-NET ENCODER   â”‚     â”‚
â”‚  â”‚ (Dataset)      â”‚ â”‚ MODULE        â”‚ â”‚ (GeoMAN)          â”‚     â”‚
â”‚  â”‚                â”‚ â”‚ (Physics)     â”‚ â”‚ (Graph NN)        â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚ â€¢ Dataset      â”‚ â”‚ â€¢ Corruption  â”‚ â”‚ â€¢ GemNetT         â”‚     â”‚
â”‚  â”‚ â€¢ ChemGraph    â”‚ â”‚ â€¢ Scoring     â”‚ â”‚ â€¢ Edge Conv       â”‚     â”‚
â”‚  â”‚ â€¢ Collate      â”‚ â”‚ â€¢ Sampling    â”‚ â”‚ â€¢ Spherical Basis â”‚     â”‚
â”‚  â”‚ â€¢ Transforms   â”‚ â”‚               â”‚ â”‚ â€¢ Radial Basis    â”‚     â”‚
â”‚  â”‚ â€¢ Properties   â”‚ â”‚               â”‚ â”‚                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PYTORCH LIGHTNING (Training Loop Management)            â”‚   â”‚
â”‚  â”‚  DiffusionLightningModule + Trainer                      â”‚   â”‚
â”‚  â”‚  - Training/Validation/Testing steps                     â”‚   â”‚
â”‚  â”‚  - Checkpoint saving/loading                             â”‚   â”‚
â”‚  â”‚  - EMA (Exponential Moving Average)                      â”‚   â”‚
â”‚  â”‚  - Distributed training (DDP)                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OUTPUT LAYER                                            â”‚   â”‚
â”‚  â”‚  - CIF files, extxyz, trajectories                       â”‚   â”‚
â”‚  â”‚  - Metrics (stability, diversity, coverage)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Technologies & Libraries

### 1. **PyTorch Geometric (PyG)**
- **Purpose**: Graph Neural Network framework
- **Role**: Basis for batch processing and message passing
- **Usage**:
  - `ChemGraph` extends `torch_geometric.data.Data`
  - `Batch` for collating multiple graphs
  - Edge creation with `radius_graph_pbc` (periodic boundary conditions)

```python
# ChemGraph is a PyG Data object with:
ChemGraph(
    atomic_numbers,      # Node features (atom types)
    pos,                 # Node positions [num_atoms, 3]
    cell,                # Periodic boundary [1, 3, 3]
    edge_index,          # Graph connectivity [2, num_edges]
    edge_attr,           # Edge features
    **properties         # Additional properties (band gap, etc.)
)
```

### 2. **PyTorch Lightning**
- **Purpose**: High-level training framework
- **Role**: Manages training loops, validation, checkpointing
- **Key Components**:
  - `DiffusionLightningModule`: Wraps the diffusion model
  - `Trainer`: Orchestrates training (epochs, devices, logging)
  - `LightningDataModule`: Handles data loading

```python
# Training loop abstraction:
class DiffusionLightningModule(pl.LightningModule):
    def training_step(batch, batch_idx):
        loss = model.calc_loss(batch)
        return loss
    
    def validation_step(batch, batch_idx):
        # Evaluation metrics
        pass
```

### 3. **Hydra (Configuration Management)**
- **Purpose**: Flexible, composable configuration system
- **Why It's Important**:
  - Avoids hardcoding hyperparameters
  - CLI overrides: `python run.py learning_rate=0.001`
  - Config composition: Combine multiple YAML files
  - Automatic output directory management
  - Type safety with dataclasses

```yaml
# conf/default.yaml
defaults:
  - data_module: mp_20          # Which dataset
  - trainer: default             # Training config
  - lightning_module: default    # Model config
  - lightning_module/diffusion_module: default  # Diffusion config

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
```

### 4. **GemNet (Graph Equivariant Message Passing Network)**
- **Purpose**: Graph neural network for materials
- **Key Features**:
  - **Equivariance**: Respects rotational/translational symmetries
  - **Triplet Message Passing**: Node-edge-node interactions
  - **Radial Basis Functions (RBF)**: Smooth distance encoding
  - **Spherical Basis**: Angular information encoding
  - **Periodic Boundary Conditions**: Native PBC support

```python
class GemNetT(nn.Module):
    """
    GemNet-T: Triplets-Only Variant
    
    Processes:
    1. Atomic numbers â†’ embeddings
    2. Distances â†’ RBF encoding
    3. Angular info â†’ Spherical basis
    4. Message passing: Node â†” Edge â†” Node
    5. Outputs: Energy, forces, stress
    """
```

### 5. **Diffusion Models (DDPM Framework)**
- **Purpose**: Generative process via iterative denoising
- **Core Idea**:
  ```
  Clean data â†’ Add noise â†’ Learn to denoise â†’ Generate
  
  Forward:   xâ‚€ â”€noiseâ†’ xâ‚ â”€noiseâ†’ ... â”€noiseâ†’ xâ‚œ
  Reverse:   xâ‚œ â”€denoiseâ†’ ... â”€denoiseâ†’ xâ‚ â”€denoiseâ†’ xâ‚€
  ```
- **Components**:
  - **Corruption**: Add noise to data (discrete + continuous)
  - **Score Model**: Learn âˆ‡log p(x_t) (gradient of log probability)
  - **Sampling**: Use predictor-corrector to reverse noise

### 6. **PyMatGen**
- **Purpose**: Materials science data structures and utilities
- **Usage**:
  - `Structure`: Crystal structure representation
  - `CifParser`: Read/write CIF files
  - `SpaceGroup`: Symmetry operations
  - `Lattice`: Crystallographic computations

---

## Data Flow & Processing

### **Complete Data Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DATA LOADING & PREPARATION                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

   CSV/CIF Files (POSCAR, CIF format)
        â†“
   CifParser (pymatgen)
        â†“
   Structure objects (pymatgen.core.Structure)
        â”‚
        â”œâ”€ Primitive structure extraction
        â”œâ”€ Niggli reduction (standardize lattice)
        â”œâ”€ Extract fractional coordinates
        â””â”€ Extract lattice matrix (3Ã—3)
        â†“
   structures_to_numpy()
        â”‚
        â”œâ”€ Flatten all atoms into single array
        â”œâ”€ Create index_offset mapping (structureâ†’atoms)
        â”œâ”€ Store properties as separate arrays
        â””â”€ Validate property dimensions
        â†“
   Cache to Disk
        â”œâ”€ pos.npy          [total_atoms, 3]
        â”œâ”€ cell.npy         [num_structures, 3, 3]
        â”œâ”€ atomic_numbers.npy [total_atoms]
        â”œâ”€ num_atoms.npy    [num_structures]
        â”œâ”€ structure_id.npy [num_structures]
        â””â”€ property.json    [num_structures]

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. DATASET BUILDING (CrystalDatasetBuilder)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

   CrystalDatasetBuilder.from_cache_path()
        â†“
   Load numpy arrays (lazy via @cached_property)
        â†“
   Load properties (band gap, space group, etc.)
        â†“
   Create CrystalDataset instance
        â”‚
        â”œâ”€ pos: [total_atoms, 3]
        â”œâ”€ cell: [num_structures, 3, 3]
        â”œâ”€ atomic_numbers: [total_atoms]
        â”œâ”€ num_atoms: [num_structures]
        â”œâ”€ structure_id: [num_structures]
        â””â”€ properties: dict[prop_name â†’ values]

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. PER-SAMPLE RETRIEVAL & TRANSFORMATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

   DataLoader[CrystalDataset].__getitem__(idx)
        â†“
   Use index_offset[idx] to get atom range
        â†“
   Extract atoms for this structure:
        â”œâ”€ positions = pos[offset:offset+num_atoms]
        â”œâ”€ atomic_nums = atomic_numbers[offset:offset+num_atoms]
        â”œâ”€ cell = cell[idx]
        â””â”€ properties = {prop: values[idx] for prop}
        â†“
   Create ChemGraph (PyG Data object)
        â”œâ”€ atomic_numbers (node features)
        â”œâ”€ pos (node coordinates, modulo 1)
        â”œâ”€ cell (graph-level property)
        â”œâ”€ num_atoms
        â””â”€ properties (space_group, band_gap, etc.)
        â†“
   Apply per-sample transforms
        â”œâ”€ Normalize/symmetrize lattice
        â”œâ”€ Data augmentation
        â””â”€ Coordinate transformations
        â†“
   Return: ChemGraph (single structure)

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. BATCH COLLATION (collate.py)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

   Multiple ChemGraphs: [graphâ‚, graphâ‚‚, ..., graphâ‚™]
        â†“
   collate() function (PyG Batch)
        â”‚
        â”œâ”€ Concatenate all pos: [total_nodes, 3]
        â”œâ”€ Concatenate all atomic_numbers: [total_nodes]
        â”œâ”€ Create batch indices for nodes
        â”œâ”€ Stack cells: [batch_size, 3, 3]
        â”œâ”€ Create batch indices for edges
        â””â”€ Concatenate properties
        â†“
   ChemGraphBatch (Dynamic PyG Batch subclass)
        â”œâ”€ pos: [total_atoms_in_batch, 3]
        â”œâ”€ atomic_numbers: [total_atoms_in_batch]
        â”œâ”€ batch: [total_atoms_in_batch] â†’ structure ID
        â”œâ”€ cell: [batch_size, 3, 3]
        â”œâ”€ num_atoms: [batch_size]
        â”œâ”€ num_graphs: batch_size
        â””â”€ properties_batch indices
        â†“
   Optional: Build edge graphs
        â”œâ”€ radius_graph_pbc(): KNN with PBC
        â”œâ”€ edge_index: [2, num_edges]
        â””â”€ edge_attr: distances, vectors, etc.

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. DIFFUSION MODEL FORWARD PASS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

   ChemGraphBatch â†’ DiffusionLightningModule
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TRAINING (training_step)                â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                                         â”‚
   â”‚ Step 1: Corruption (Add Noise)          â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
   â”‚  Noisy pos = pos + Î±Â·Îµ                  â”‚
   â”‚  Noisy atomic_nums = discrete corrupt   â”‚
   â”‚  Noisy cell = cell + Î²Â·Îµ                â”‚
   â”‚  Sample timestep t âˆˆ [0,T]              â”‚
   â”‚                                         â”‚
   â”‚ Step 2: Score Model (GemNet)            â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
   â”‚  pred_atom_types = ScoreModel(noisy, t) â”‚
   â”‚  pred_pos_noise = ScoreModel(noisy, t)  â”‚
   â”‚  pred_cell_noise = ScoreModel(noisy, t) â”‚
   â”‚                                         â”‚
   â”‚ Step 3: Loss Computation                â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
   â”‚  L = MSE(pred_noise, true_noise)        â”‚
   â”‚  + cross_entropy(atom_logits, true_atoms)
   â”‚  + property_matching_loss               â”‚
   â”‚                                         â”‚
   â”‚ Step 4: Backward Pass (PyTorch)         â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
   â”‚  âˆ‡Î¸ L â†’ Update weights                  â”‚
   â”‚                                         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   Return: loss, metrics

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. GENERATION (INFERENCE)                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

   CrystalGenerator.generate()
        â†“
   Load checkpoint (pretrained weights)
        â†“
   ConditionLoader (creates conditioning batches)
        â”œâ”€ Num atoms distribution
        â”œâ”€ Properties (e.g., "band_gap > 2.0")
        â””â”€ Chemical system constraints
        â†“
   PredictorCorrector Sampler
        â”‚
        â”œâ”€ Start: xâ‚œ âˆˆ ğ’©(0, I)  [random noise]
        â”‚
        â”œâ”€ For t = T to 0:
        â”‚    â”‚
        â”‚    â”œâ”€ Predictor step:
        â”‚    â”‚    s(xâ‚œ, t) = ScoreModel(xâ‚œ, t)
        â”‚    â”‚    xâ‚œâ‚‹â‚ = xâ‚œ + drift(s) + diffusion
        â”‚    â”‚
        â”‚    â””â”€ Corrector step:
        â”‚         Langevin dynamics: refine xâ‚œâ‚‹â‚
        â”‚
        â””â”€ Final: xâ‚€ â‰ˆ clean structure
        â†“
   Convert to Cartesian coordinates
        â†“
   Post-process:
   â”œâ”€ Convert to Ã… units
   â”œâ”€ Create Structure object (pymatgen)
   â””â”€ Validate (check PBC, etc.)
        â†“
   Save:
   â”œâ”€ CIF files (human-readable)
   â”œâ”€ extxyz (ASE format)
   â”œâ”€ Trajectories (full denoising path)
   â””â”€ Metadata JSON

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Classes & Components

### **1. ChemGraph (Atomic Structure Representation)**
```python
class ChemGraph(torch_geometric.data.Data):
    """
    PyG Data object for crystal structures.
    
    Attributes:
    - atomic_numbers: [num_atoms] - atom type (1-indexed)
    - pos: [num_atoms, 3] - fractional coordinates
    - cell: [1, 3, 3] - lattice matrix
    - edge_index: [2, num_edges] - graph edges
    - num_atoms: scalar - atoms in structure
    - num_nodes: scalar - same as num_atoms (PyG convention)
    - **properties: band_gap, space_group, etc.
    
    Methods:
    - replace(**kwargs): Create copy with updated fields
    - get_batch_idx(field_name): Get batch indices
    """
```

**Why Frozen?**
```python
def __setattr__(self, attr, value):
    if self.__dict__.get("_frozen", False):
        raise AttributeError("Use replace() instead")
```
Prevents accidental mutations; use `replace()` for immutability.

### **2. CrystalDataset (Data Loading)**
```python
class CrystalDataset(BaseDataset):
    """
    Efficient numpy-based dataset with flattened arrays.
    
    Storage:
    - pos: [total_atoms, 3] (single flat array)
    - atomic_numbers: [total_atoms]
    - cell: [num_structures, 3, 3]
    - num_atoms: [num_structures] (per-structure counts)
    - index_offset: [num_structures] â†’ atom indices
    
    Access Pattern:
    __getitem__(idx):
        offset = index_offset[idx]
        count = num_atoms[idx]
        structure_atoms = pos[offset:offset+count]
    
    Methods:
    - subset(indices): Create subset dataset
    - repeat(n): Duplicate dataset n times
    - get_properties_dict(idx): Return properties as tensors
    """
```

### **3. CrystalDatasetBuilder (Factory Pattern)**
```python
class CrystalDatasetBuilder:
    """
    Manages dataset loading, caching, and property management.
    
    Workflow:
    1. from_csv() - Parse CIF/POSCAR files
    2. structures_to_numpy() - Convert to flat arrays
    3. Cache to disk - Save .npy and .json
    4. from_cache_path() - Lazy reload
    5. build() - Instantiate CrystalDataset
    
    Properties:
    - Sparse support: Missing properties â†’ NaN
    - Dynamic addition: add_property_to_cache()
    - Validation: Check dimensions match
    """
```

### **4. DiffusionLightningModule (Training Manager)**
```python
class DiffusionLightningModule(pl.LightningModule):
    """
    PyTorch Lightning wrapper for diffusion model.
    
    Attributes:
    - diffusion_module: Core diffusion logic
    - optimizer_partial: Optimizer factory
    - scheduler_partials: LR scheduler factories
    
    Methods:
    - training_step(batch, idx): Loss calculation
    - validation_step(batch, idx): Evaluation
    - configure_optimizers(): Setup Adam, schedulers
    - load_from_checkpoint(): Restore from saved state
    
    Features:
    - EMA (Exponential Moving Average) for stability
    - Automatic checkpoint saving (best, last)
    - Multi-GPU support (DDP)
    - Distributed validation
    """
```

### **5. DiffusionModule (Core Physics)**
```python
class DiffusionModule(nn.Module):
    """
    Denoising diffusion probabilistic model.
    
    Components:
    - model: ScoreModel (GemNet) - learns âˆ‡log p(x)
    - corruption: MultiCorruption - defines noise schedule
    - loss_fn: Loss function
    - timestep_sampler: Sample t âˆˆ [0, T]
    
    Process:
    1. Forward: xâ‚€ â†’ noisy_x_t (via corruption)
    2. Score: Å = model(noisy_x_t, t)
    3. Loss: L = ||Å - âˆ‡log p(x_t||xâ‚€)||Â²
    4. Optimize: Î¸ â† Î¸ - âˆ‡Î¸ L
    
    Methods:
    - calc_loss(batch): Compute training loss
    - _corrupt_batch(): Add noise
    """
```

### **6. GemNetT (Score Model/Denoiser)**
```python
class GemNetT(nn.Module):
    """
    Graph Equivariant Message Passing Network.
    
    Architecture:
    INPUT: noisy structure (pos, cell, atomic_nums) + timestep
    
    â”œâ”€ Timestep Encoding
    â”‚  â””â”€ sin/cos positional encoding of t
    â”‚
    â”œâ”€ Atom Embedding
    â”‚  â””â”€ atomic_number â†’ vector
    â”‚
    â”œâ”€ Edge Creation
    â”‚  â””â”€ radius_graph_pbc (k-nearest neighbors + PBC)
    â”‚
    â”œâ”€ Interaction Blocks (stacked)
    â”‚  â”œâ”€ Radial Basis Functions (RBF)
    â”‚  â”‚  â””â”€ Encode distances smoothly
    â”‚  â”œâ”€ Spherical Basis
    â”‚  â”‚  â””â”€ Encode angles
    â”‚  â””â”€ Triplet Message Passing
    â”‚     â”œâ”€ Node â†’ Edge messages
    â”‚     â””â”€ Edge â†’ Node aggregation
    â”‚
    â””â”€ Output Block
       â”œâ”€ Predict noise in positions
       â”œâ”€ Predict noise in cell
       â”œâ”€ Predict logits for atom types
       â””â”€ Predict forces/stress (optional)
    
    Key Properties:
    - Equivariance: E(RÂ·x) = RÂ·E(x) for rotations R
    - Covariance: E(x + Ï„) = E(x) + Ï„ (translation)
    - Periodic: Handles PBC automatically
    """
```

### **7. Collate Function (Batching)**
```python
def collate(pytree: PyTree[ChemGraph]) -> ChemGraphBatch:
    """
    Merge multiple ChemGraphs into batch using PyG.
    
    Input: [graphâ‚, graphâ‚‚, ..., graphâ‚™]
    
    Process:
    1. Concatenate node features
       pos: [nâ‚+nâ‚‚+...+nâ‚™, 3]
       atomic_numbers: [nâ‚+nâ‚‚+...+nâ‚™]
    
    2. Create batch indices
       batch: [nâ‚+nâ‚‚+...+nâ‚™] â†’ which graph each atom belongs to
    
    3. Stack graph-level features
       cell: [n, 3, 3] â†’ [batch_size, 3, 3]
       num_atoms: [n] â†’ [batch_size]
    
    4. Optional: Build edge graphs
       radius_graph_pbc + periodic distance matrix
    
    Result: ChemGraphBatch object with batch indices for:
    - Node attributes (pos, atomic_numbers)
    - Edge attributes
    - Graph attributes (cell, properties)
    """
```

---

## Configuration System (Hydra)

### **Hydra: Why It Matters**

Hydra provides **declarative configuration management** instead of scattered hyperparameters:

```bash
# Without Hydra: hardcoded in code
python train.py  # hidden hyperparams in code

# With Hydra: explicit and overridable
python train.py learning_rate=0.001 batch_size=32 dataset=mp_20
```

### **Configuration Hierarchy**

```
mattergen/conf/
â”œâ”€â”€ default.yaml              # Main entry point
â”‚   â””â”€â”€ Specifies defaults for all subsystems
â”œâ”€â”€ finetune.yaml             # Fine-tuning overrides
â”œâ”€â”€ csp.yaml                  # Crystal structure prediction mode
â”œâ”€â”€ data_module/
â”‚   â”œâ”€â”€ mp_20.yaml            # MP-20 dataset config
â”‚   â”œâ”€â”€ alex_mp_20.yaml       # Alex-MP-20 dataset config
â”‚   â””â”€â”€ custom.yaml           # Custom dataset config
â”œâ”€â”€ lightning_module/
â”‚   â”œâ”€â”€ default.yaml          # Base model config
â”‚   â””â”€â”€ diffusion_module/
â”‚       â”œâ”€â”€ default.yaml      # Diffusion config
â”‚       â”œâ”€â”€ model/
â”‚       â”‚   â”œâ”€â”€ mattergen.yaml   # GemNet params
â”‚       â”‚   â””â”€â”€ baseline.yaml
â”‚       â””â”€â”€ corruption/
â”‚           â””â”€â”€ default.yaml  # Noise schedule
â””â”€â”€ trainer/
    â””â”€â”€ default.yaml          # PyTorch Lightning Trainer config
```

### **Config Resolution Process**

```
1. Load default.yaml
2. Load all defaults: (data_module, trainer, lightning_module, ...)
3. Merge YAML files â†’ base config
4. Parse CLI args â†’ overrides
5. Apply overrides to base config
6. Validate against Config dataclass
7. Instantiate all objects via Hydra
```

### **Example: default.yaml**
```yaml
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}  # Dynamic output dir

auto_resume: True

defaults:
  - data_module: mp_20                    # Load mp_20 dataset config
  - trainer: default                      # Default trainer settings
  - lightning_module: default             # Default model config
  - lightning_module/diffusion_module: default  # Default diffusion
  - lightning_module/diffusion_module/model: mattergen  # GemNet params
  - lightning_module/diffusion_module/corruption: default  # Noise schedule
```

### **Hydra Instantiation**

Hydra converts YAML â†’ Python objects:

```yaml
# conf/data_module/mp_20.yaml
_target_: mattergen.common.data.data_module.CrystalDataModule
dataset_name: mp_20
split: train
batch_size: 64
num_workers: 4
```

```python
# In code:
from hydra.utils import instantiate

data_module = instantiate(cfg.data_module)
# â†’ Creates: CrystalDataModule(dataset_name="mp_20", ...)
```

### **Key Hydra Features Used in MatterGen**

| Feature | Usage |
|---------|-------|
| **Composition** | Combine configs from different domains |
| **CLI Overrides** | `python train.py learning_rate=0.001` |
| **Interpolation** | `${other_key}` references within YAML |
| **Defaults List** | Order matters - later overrides earlier |
| **Instantiation** | Convert configs â†’ Python objects |
| **Output Dir Management** | Auto-create `outputs/YYYY-MM-DD/HH-MM-SS/` |
| **Config Validation** | Type-checked with dataclasses |

---

## Execution Workflows

### **1. TRAIN Workflow**

```
$ python mattergen/scripts/run.py [config_overrides]

1. INITIALIZATION
   â”œâ”€ Hydra loads default.yaml + overrides
   â”œâ”€ Config merged and validated
   â””â”€ Output directory created
        â†“

2. DATA MODULE SETUP
   â”œâ”€ instantiate(cfg.data_module)
   â”œâ”€ CrystalDataModule created
   â”‚  â”œâ”€ setup(stage='fit')
   â”‚  â”œâ”€ Load training dataset
   â”‚  â”œâ”€ Load validation dataset
   â”‚  â””â”€ Create DataLoaders
   â””â”€ Batches ready for training
        â†“

3. MODEL INITIALIZATION
   â”œâ”€ instantiate(cfg.lightning_module)
   â”œâ”€ Creates DiffusionLightningModule
   â”‚  â”œâ”€ GemNetT score model
   â”‚  â”œâ”€ MultiCorruption (noise schedule)
   â”‚  â”œâ”€ DiffusionModule (forward diffusion)
   â”‚  â””â”€ Loss functions
   â””â”€ Model on device (GPU/CPU)
        â†“

4. TRAINER SETUP
   â”œâ”€ instantiate(cfg.trainer)
   â”œâ”€ PyTorch Lightning Trainer
   â”‚  â”œâ”€ Num epochs, devices, precision
   â”‚  â”œâ”€ Checkpoint callbacks
   â”‚  â”œâ”€ Validation frequency
   â”‚  â””â”€ EMA callbacks
   â””â”€ Logger setup (TensorBoard, Weights&Biases)
        â†“

5. TRAINING LOOP
   â”œâ”€ For epoch = 0 to num_epochs:
   â”‚  â”‚
   â”‚  â”œâ”€ FOR EACH BATCH:
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ training_step(batch):
   â”‚  â”‚  â”‚  â”œâ”€ Batch collation
   â”‚  â”‚  â”‚  â”œâ”€ Forward pass (add noise + denoise)
   â”‚  â”‚  â”‚  â”œâ”€ Loss computation
   â”‚  â”‚  â”‚  â”œâ”€ Backward pass
   â”‚  â”‚  â”‚  â””â”€ Optimizer.step()
   â”‚  â”‚  â”‚
   â”‚  â”‚  â””â”€ Log training metrics
   â”‚  â”‚
   â”‚  â”œâ”€ VALIDATION (every N steps):
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ FOR EACH VAL BATCH:
   â”‚  â”‚  â”‚  â”œâ”€ validation_step(batch)
   â”‚  â”‚  â”‚  â”œâ”€ Compute validation loss
   â”‚  â”‚  â”‚  â””â”€ Track metrics
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ Aggregate validation metrics
   â”‚  â”‚  â”‚
   â”‚  â”‚  â””â”€ Save best checkpoint
   â”‚  â”‚
   â”‚  â””â”€ Update learning rate scheduler
   â”‚
   â””â”€ Save final checkpoint
        â†“

6. POST-TRAINING
   â”œâ”€ Save config.yaml in checkpoint dir
   â”œâ”€ Save training metrics
   â”œâ”€ Create checkpoint archive
   â””â”€ Training complete!

KEY FILES INVOLVED:
â”œâ”€ mattergen/scripts/run.py          [Entry point]
â”œâ”€ mattergen/diffusion/run.py        [Main training logic]
â”œâ”€ mattergen/diffusion/lightning_module.py  [Training step]
â”œâ”€ mattergen/diffusion/diffusion_module.py  [Loss computation]
â”œâ”€ mattergen/common/data/data_module.py    [Data loading]
â”œâ”€ mattergen/common/data/dataset.py        [Dataset class]
â””â”€ mattergen/conf/                   [Config files]
```

### **2. GENERATE Workflow**

```
$ mattergen-generate results/ --pretrained-name=mattergen_base --batch_size=16

1. CHECKPOINT LOADING
   â”œâ”€ Load checkpoint from:
   â”‚  â”œâ”€ Hugging Face Hub (if pretrained-name)
   â”‚  â””â”€ Or local path (if model_path)
   â”‚
   â”œâ”€ MatterGenCheckpointInfo.from_hf_hub("mattergen_base")
   â”‚  â”œâ”€ Download from HuggingFace
   â”‚  â””â”€ Extract config.yaml
   â”‚
   â””â”€ DiffusionLightningModule.load_from_checkpoint()
      â”œâ”€ Load state_dict
      â”œâ”€ Reconstruct model architecture
      â””â”€ Model in eval mode
        â†“

2. SAMPLING CONFIGURATION
   â”œâ”€ Load sampling config (default.yaml)
   â”œâ”€ ConditionLoader setup:
   â”‚  â”œâ”€ Num atoms distribution (if unconditional)
   â”‚  â””â”€ Conditioning info (if conditional)
   â”‚
   â””â”€ Create batches:
      â”œâ”€ Batch 1: 16 structures
      â”œâ”€ Batch 2: 16 structures
      â””â”€ ... (num_batches times)
        â†“

3. SAMPLING SETUP
   â”œâ”€ PredictorCorrector sampler
   â”‚  â”œâ”€ Timesteps: 50, 100, 250 (configurable)
   â”‚  â””â”€ Noise schedule: linear, quadratic, etc.
   â”‚
   â””â”€ Optional guidance:
      â”œâ”€ Classifier-free guidance
      â”œâ”€ Scaling factor Î²
      â””â”€ Property constraints
        â†“

4. DENOISING LOOP (Per Batch)
   â”œâ”€ Initialize: xâ‚œ ~ ğ’©(0, I)  [random noise]
   â”‚  â”œâ”€ Positions: [batch_size*avg_atoms, 3]
   â”‚  â”œâ”€ Cell: [batch_size, 3, 3]
   â”‚  â””â”€ Atomic numbers: [batch_size*avg_atoms]
   â”‚
   â”œâ”€ FOR t = T down to 0 (descending):
   â”‚  â”‚
   â”‚  â”œâ”€ PREDICTOR STEP (Reverse SDE)
   â”‚  â”‚  â”œâ”€ Score = model(xâ‚œ, t)
   â”‚  â”‚  â”‚  (GemNet forward pass)
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ Update positions:
   â”‚  â”‚  â”‚  xâ‚œâ‚‹â‚ = xâ‚œ + drift*dt + Ïƒ*dw
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ Update cell:
   â”‚  â”‚  â”‚  cellâ‚œâ‚‹â‚ = cellâ‚œ + drift*dt
   â”‚  â”‚  â”‚
   â”‚  â”‚  â”œâ”€ Update atom types:
   â”‚  â”‚  â”‚  logits â†’ sample with temperature
   â”‚  â”‚  â”‚
   â”‚  â”‚  â””â”€ Optional guidance step
   â”‚  â”‚
   â”‚  â”œâ”€ CORRECTOR STEP (Langevin)
   â”‚  â”‚  â””â”€ Refine xâ‚œâ‚‹â‚ via auxiliary SDE
   â”‚  â”‚
   â”‚  â””â”€ Log trajectory (if record_trajectories=True)
   â”‚
   â””â”€ Final: xâ‚€ â‰ˆ clean structure
        â†“

5. POST-PROCESSING
   â”œâ”€ Convert to Cartesian coordinates
   â”‚  â””â”€ pos_cart = pos_frac @ cell
   â”‚
   â”œâ”€ Wrap to unit cell (0 â‰¤ pos < 1)
   â”‚
   â”œâ”€ Create Structure objects (pymatgen)
   â”‚  â”œâ”€ lattice = cell
   â”‚  â”œâ”€ species = [elem(Z) for Z in atomic_numbers]
   â”‚  â””â”€ coords = pos_cart
   â”‚
   â””â”€ Validate structures
      â”œâ”€ Check PBC
      â”œâ”€ Check atom overlaps
      â””â”€ Remove invalid structures
        â†“

6. OUTPUT SAVING
   â”œâ”€ Generated structures:
   â”‚  â”œâ”€ CIF files (one per structure)
   â”‚  â”œâ”€ extxyz format (all in one file)
   â”‚  â””â”€ Zipped archive
   â”‚
   â”œâ”€ Trajectories (if record_trajectories=True):
   â”‚  â”œâ”€ Full denoising path for each structure
   â”‚  â””â”€ Time-evolved positions
   â”‚
   â””â”€ Metadata:
      â”œâ”€ generation_config.json
      â”œâ”€ sampled_properties.json
      â””â”€ statistics.json

KEY FILES INVOLVED:
â”œâ”€ mattergen/scripts/generate.py            [Entry point]
â”œâ”€ mattergen/generator.py                   [Main generation logic]
â”œâ”€ mattergen/diffusion/sampling/pc_sampler.py  [Denoising loop]
â”œâ”€ mattergen/denoiser.py                    [GemNet wrapper]
â”œâ”€ mattergen/common/utils/eval_utils.py    [Post-processing]
â””â”€ mattergen/conf/sampling_conf/            [Sampling config]
```

### **3. FINETUNE Workflow**

```
$ python mattergen/scripts/finetune.py \
    --pretrained-name=mattergen_base \
    --property-name=band_gap \
    --train-data-path=/path/to/data.csv

1. BASE MODEL LOADING
   â”œâ”€ Load pretrained checkpoint
   â”œâ”€ MatterGenCheckpointInfo.from_hf_hub()
   â”‚  â”œâ”€ Download weights
   â”‚  â””â”€ Extract config
   â”‚
   â””â”€ Parse original config:
      â”œâ”€ GemNet architecture
      â”œâ”€ Corruption schedule
      â””â”€ Loss functions
        â†“

2. ADAPTER INITIALIZATION
   â”œâ”€ Create GemNetTCtrl (controlled variant)
   â”‚  â”œâ”€ Same as GemNetT + adapter layers
   â”‚  â””â”€ Learnable property embeddings
   â”‚
   â”œâ”€ Configure property to condition on
   â”‚  â””â”€ e.g., band_gap with embedding size 16
   â”‚
   â”œâ”€ Transfer weights from pretrained:
   â”‚  â”œâ”€ Copy matching parameters
   â”‚  â”œâ”€ New adapter layers initialized randomly
   â”‚  â””â”€ Freeze or fine-tune base weights
   â”‚
   â””â”€ Setup new property embeddings
      â”œâ”€ PropertyEmbedding module
      â””â”€ Map property values â†’ vectors
        â†“

3. DATASET PREPARATION
   â”œâ”€ Load CSV with:
   â”‚  â”œâ”€ CIF/POSCAR structures
   â”‚  â”œâ”€ Material IDs
   â”‚  â””â”€ Property values (band_gap, etc.)
   â”‚
   â”œâ”€ structures_to_numpy()
   â”‚  â””â”€ Convert to flat arrays
   â”‚
   â”œâ”€ Add property to cache
   â”‚  â””â”€ PropertyValues.to_json()
   â”‚
   â””â”€ Create DataLoader
      â”œâ”€ Batch structures
      â””â”€ Include property labels
        â†“

4. TRAINING LOOP (Similar to train, but)
   â”œâ”€ Lower learning rate (transfer learning)
   â”‚  â””â”€ Usually 10x smaller
   â”‚
   â”œâ”€ Optional: Freeze base layers
   â”‚  â”œâ”€ Only train property embeddings
   â”‚  â””â”€ Or train all with lower LR
   â”‚
   â”œâ”€ New loss includes:
   â”‚  â”œâ”€ Reconstruction loss (as before)
   â”‚  â”œâ”€ Property matching loss
   â”‚  â”‚  L_prop = ||model(x, t) - target||Â²
   â”‚  â””â”€ Combined loss: L_total = L_recon + Î»*L_prop
   â”‚
   â””â”€ Validation on held-out property data
        â†“

5. CHECKPOINT SAVING
   â”œâ”€ Save adapter weights
   â”œâ”€ Save property embeddings
   â”œâ”€ Save config with new property
   â””â”€ New checkpoint ready for generation!

KEY FILES INVOLVED:
â”œâ”€ mattergen/scripts/finetune.py            [Entry point]
â”œâ”€ mattergen/adapter.py                     [Adapter logic]
â”œâ”€ mattergen/property_embeddings.py         [Property conditioning]
â”œâ”€ mattergen/common/data/data_module.py    [Data loading]
â””â”€ mattergen/conf/finetune.yaml            [Config]
```

### **4. EVALUATE Workflow**

```
$ mattergen-evaluate results/generated.extxyz \
    --relax \
    --reference-dataset-path=/path/to/mp_20

1. STRUCTURE LOADING
   â”œâ”€ Load from:
   â”‚  â”œâ”€ CIF files (directory)
   â”‚  â”œâ”€ extxyz (single file)
   â”‚  â””â”€ ASE trajectory
   â”‚
   â””â”€ Parse with pymatgen/ASE
      â””â”€ Create Structure objects
        â†“

2. OPTIONAL: STRUCTURE RELAXATION
   â”œâ”€ Use MACE or MatterSim potential
   â”‚
   â”œâ”€ Relax atomic positions
   â”‚  â””â”€ Minimize forces
   â”‚
   â”œâ”€ Relax cell
   â”‚  â””â”€ Minimize stress
   â”‚
   â””â”€ Extract relaxed energy
        â†“

3. EVALUATION METRICS
   â”œâ”€ Validity:
   â”‚  â”œâ”€ Check for overlapping atoms
   â”‚  â”œâ”€ Check composition feasibility
   â”‚  â””â”€ Check lattice parameters
   â”‚
   â”œâ”€ Stability:
   â”‚  â”œâ”€ Energy above hull (if reference provided)
   â”‚  â”œâ”€ Phonon frequencies (if computed)
   â”‚  â””â”€ Formation energy
   â”‚
   â”œâ”€ Diversity:
   â”‚  â”œâ”€ Maximum pairwise distance (structure distance)
   â”‚  â”œâ”€ Composition distribution
   â”‚  â””â”€ Crystal system distribution
   â”‚
   â”œâ”€ Novelty:
   â”‚  â”œâ”€ Comparison with reference dataset
   â”‚  â”œâ”€ Structure matching (tolerance)
   â”‚  â””â”€ Novel compositions
   â”‚
   â””â”€ Coverage:
      â”œâ”€ Distribution match with training set
      â””â”€ Property prediction accuracy
        â†“

4. COMPARISON WITH REFERENCE
   â”œâ”€ Load reference dataset (MP-20, ICSD, etc.)
   â”‚
   â”œâ”€ For each generated structure:
   â”‚  â”œâ”€ Find nearest neighbors in reference
   â”‚  â”œâ”€ Structure match (default: disordered)
   â”‚  â””â”€ Calculate metrics
   â”‚
   â””â”€ Aggregate statistics
        â†“

5. OUTPUT
   â”œâ”€ JSON with all metrics:
   â”‚  â”œâ”€ num_valid
   â”‚  â”œâ”€ num_stable (Ehull < threshold)
   â”‚  â”œâ”€ avg_distance_to_reference
   â”‚  â”œâ”€ num_duplicates
   â”‚  â””â”€ property_MAE (if properties provided)
   â”‚
   â”œâ”€ CSV with per-structure metrics
   â”‚
   â””â”€ Optionally save relaxed structures

KEY FILES INVOLVED:
â”œâ”€ mattergen/scripts/evaluate.py            [Entry point]
â”œâ”€ mattergen/evaluation/evaluate.py         [Main logic]
â”œâ”€ mattergen/evaluation/utils/metrics.py   [Metrics computation]
â”œâ”€ mattergen/evaluation/utils/structure_matcher.py  [Matching]
â””â”€ mattergen/common/utils/eval_utils.py    [Utilities]
```

---

## Important Files & Their Roles

### **Core Architecture Files**

| File | Purpose | Key Classes |
|------|---------|-------------|
| `dataset.py` | Data loading & caching | `CrystalDataset`, `CrystalDatasetBuilder` |
| `chemgraph.py` | Structure representation | `ChemGraph` |
| `collate.py` | Batch creation | `collate()` function |
| `diffusion_module.py` | Core diffusion physics | `DiffusionModule` |
| `lightning_module.py` | Training orchestration | `DiffusionLightningModule` |
| `denoiser.py` | Score model wrapper | `get_chemgraph_from_denoiser_output()` |
| `generator.py` | Generation orchestration | `CrystalGenerator` |
| `property_embeddings.py` | Property conditioning | `ChemicalSystemMultiHotEmbedding` |

### **Geometry & Physics Files**

| File | Purpose | Key Functions |
|------|---------|----------------|
| `gemnet.py` | Graph NN architecture | `GemNetT` class |
| `gemnet_ctrl.py` | Adaptive GemNet | `GemNetTCtrl` class |
| `layers/` | Low-level GNN layers | Radial basis, spherical basis, convolutions |
| `diffusion_module.py` | Corruption & scoring | `DiffusionModule`, `MultiCorruption` |
| `sampling/pc_sampler.py` | Denoising loop | `PredictorCorrector` |

### **Utility Files**

| File | Purpose | Key Utilities |
|------|---------|---------------|
| `data_utils.py` | Coordinate conversions | `frac_to_cart_coords_with_lattice()` |
| `eval_utils.py` | Post-processing | `make_structure()`, `save_structures()` |
| `globals.py` | Constants | `PROPERTY_SOURCE_IDS`, `SELECTED_ATOMIC_NUMBERS` |
| `structure_matcher.py` | Structure comparison | `DefaultDisorderedStructureMatcher` |

### **Configuration Files**

| File | Purpose |
|------|---------|
| `conf/default.yaml` | Main entry point config |
| `conf/data_module/*.yaml` | Dataset selection |
| `conf/lightning_module/*.yaml` | Model architecture |
| `conf/trainer/*.yaml` | Training parameters |
| `sampling_conf/*.yaml` | Generation parameters |

---

## Advanced Topics

### **Batch Processing with PyTorch Geometric**

```python
# Single structure (ChemGraph):
graph = ChemGraph(
    atomic_numbers=[6, 8, 1],      # 3 atoms
    pos=[[0.1, 0.2, 0.3],          # fractional coords
         [0.5, 0.5, 0.5],
         [0.0, 0.0, 0.0]],
    cell=[[5, 0, 0],               # lattice (angstroms)
          [0, 5, 0],
          [0, 0, 5]]
)

# Multiple structures in batch:
batch = collate([graph1, graph2, graph3])

# batch.pos: [n1+n2+n3, 3] - concatenated atoms
# batch.batch: [n1+n2+n3] - which structure each atom belongs to
# batch.num_graphs: 3

# Access by structure:
structure_1_pos = batch.pos[batch.batch == 0]  # All atoms in structure 1
```

### **Equivariance in GemNet**

```python
"""
GemNet maintains equivariance to:

1. Rotations (3D rotations in space)
   E(RÂ·x) = RÂ·E(x)
   If you rotate structure, embeddings rotate accordingly

2. Translations (doesn't matter, PBC handled)
   E(x + Ï„) â‰ˆ E(x)  (modulo periodicity)

3. Permutations (order of atoms)
   E([x1, x2, x3]) = E([x3, x1, x2])
   Permutation invariant via message passing

This is CRITICAL for materials:
- Learned features respect physical symmetries
- Model generalizes better to unseen structures
- Forces, stresses are naturally covariant
"""
```

### **Property Conditioning Mechanism**

```python
# How properties steer generation:

1. PROPERTY EMBEDDING (during training):
   band_gap_value = 2.5 eV
   â†“
   band_gap_embedding = PropertyEmbedding(2.5)  # â†’ [16]
   â†“
   model inputs this embedding at each GemNet layer

2. UNCONDITIONAL EMBEDDING (for flexibility):
   If band_gap is NOT specified, use:
   special_vector = torch.ones(16)  # all ones or learnable
   
   This allows model to learn: "with this vector, any band gap is OK"

3. CLASSIFIER-FREE GUIDANCE (during generation):
   Two predictions:
   - With condition: Å_cond = model(x, t, embed_condition)
   - Without condition: Å_uncond = model(x, t, embed_null)
   
   Blended: Å = Å_uncond + Î²Â·(Å_cond - Å_uncond)
   
   Higher Î² â†’ stronger conditioning effect
```

### **Noise Schedule (Corruption)**

```python
# During training:
t âˆˆ [0, 1]  (normalized timestep)

Corruption adds noise:
x_t = âˆš(á¾±_t) Â· x_0 + âˆš(1-á¾±_t) Â· Îµ

where á¾±_t = âˆ(1-Î²_i)  is cumulative variance

Î²_t can be:
- Linear: Î²_t = 0.0001 + t * (0.02 - 0.0001)
- Quadratic: Î²_t = (0.0001 + t * (0.02 - 0.0001))Â²
- Cosine: Î²_t = sinÂ²(Ï€Â·t/2)

This schedule:
- Early (small t): mostly signal + little noise
- Late (large t): mostly noise + little signal

Model learns: "Here's noisy structure. Remove this much noise."
```

### **Loss Functions in MatterGen**

```python
# Multi-part loss:

L_total = L_positions + L_cell + L_atoms + L_properties

1. L_positions:
   MSE between predicted and true noise in coordinates
   L = ||Ïƒ_pred - Ïƒ_true||Â²

2. L_cell:
   MSE for lattice noise prediction
   L = ||cell_pred - cell_true||Â²

3. L_atoms:
   Cross-entropy for discrete atom types
   L = -Î£_i log(p_i[true_atom_type])

4. L_properties (if conditioning on properties):
   Match property predictions to target values
   L_prop = ||property_pred - target||Â²

Final: L = Î£ Î±_i * L_i  (weighted sum)
```

---

## Summary: The Complete Picture

```
MatterGen creates materials by:

1. TRAINING:
   â”œâ”€ Load real crystal structures (MP-20, etc.)
   â”œâ”€ Add noise iteratively (forward diffusion)
   â”œâ”€ Train neural network to reverse noise (denoising)
   â”œâ”€ Use GemNet (respects physics symmetries)
   â””â”€ Condition on properties (band gap, space group, etc.)

2. GENERATION:
   â”œâ”€ Start with pure random noise
   â”œâ”€ Iteratively remove noise (reverse diffusion)
   â”œâ”€ Use trained network to guide denoising
   â”œâ”€ Apply property constraints (classifier-free guidance)
   â””â”€ End up with plausible new structures

3. WHY THIS WORKS:
   â”œâ”€ Diffusion â‰ˆ Maximum likelihood learning (proven mathematically)
   â”œâ”€ GemNet respects physical symmetries
   â”œâ”€ Property conditioning allows controlled generation
   â”œâ”€ Iterative denoising = high quality
   â””â”€ Probabilistic â†’ uncertainty quantification

4. KEY ADVANTAGES:
   â”œâ”€ Works for arbitrary materials (not limited to test set)
   â”œâ”€ Conditionable on multiple properties
   â”œâ”€ Physics-aware (equivariant) architecture
   â”œâ”€ Fast inference (50-250 steps to denoise)
   â””â”€ Publicly available pre-trained checkpoints
```

